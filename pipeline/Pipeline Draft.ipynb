{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf1a4dd-8240-4092-bbcf-a05b90ed6cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3326e84-434f-4156-9466-0e93680bc174",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/clean_EventsAdapt_SentenceSet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c7e920-957e-4179-b789-a03341e0bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29d071b5-b871-4028-b7ca-58772ec67354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch._C._cuda_getDeviceCount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6fb607-2299-4909-82db-af0447ccea6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20f6a2fa8644619934d7a8548928b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c512f0ab7ff49819b7e737f7b28b3c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32251ebccb34470ba4096bb69793864b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b1d196b4ed4065ad7ceea1cd7660e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4b0f8decdb4bc38f4604624963c712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b694bc8a18664cadb1915f49e52b205d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce9b47f9c5244c39ae0e9245c627107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4e3abccdb047feb28e1fd9d361a571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1e70792b9b4a38a3d7675f1fb0f054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740ec3fe8bcf424cb9cf9cfc73a7ea61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/areebkhalfay/Repositories/psych-research/lib/python3.12/site-packages/accelerate/utils/modeling.py:1536: UserWarning: Current model requires 256 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4a117d880442709fc7d7320bd76694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B\"  # Replace with any HF chat model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, return_dict_in_generate=True, output_scores=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11cbd438-398e-4e24-bc9c-1efbfbd1c19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prompt = df.iloc[0]['Sentence'] + \" Is this sentence plausible? Answer only with 1 for yes or 0 for no\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to('cuda').input_ids\n",
    "output = model.generate(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "311ce4c0-de5b-4e8b-a648-32f35268bde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The raider caught the illness. Is this sentence plausible? Answer only with 1 for yes or 0 for no.\n",
      "\n",
      "The raider caught the illness. Is this sentence plausible? Answer only with 1 for\n"
     ]
    }
   ],
   "source": [
    "generated_ids = output.sequences[0]\n",
    "response_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "print(\"Response:\", response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "361d8e60-0bf7-4e2a-988f-c192c632a2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_scores = model.compute_transition_scores(\n",
    "    output.sequences, output.scores, normalize_logits=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e994f4c5-8674-4ad8-a431-2b5e54c365d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = torch.log_softmax(torch.stack(output.scores), dim=-1)\n",
    "generated_tokens = generated_ids[input_ids.shape[1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "408838a7-601c-4402-8a83-65733d20aa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Probabilities: [-1.745070457458496, -0.0014392504235729575, -2.4316742420196533, -0.8991115093231201, -0.4054230749607086, -0.10869777202606201, -0.1516532301902771, -0.032304685562849045, -1.0999172925949097, -2.0126843452453613, -0.00022897482267580926, -1.2179912328720093, -0.23104265332221985, -0.34323638677597046, -0.05943372845649719, -0.024750739336013794, -0.0993984043598175, -0.4383017420768738, -0.4482728838920593, -0.00020418466010596603]\n"
     ]
    }
   ],
   "source": [
    "token_log_probs = [log_probs[i, :, token_id].item() for i, token_id in enumerate(generated_tokens)]\n",
    "print(\"Log Probabilities:\", token_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8078e0f-bf4f-4a46-ad14-e5d139d1a09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Log Probability: -11.7508\n"
     ]
    }
   ],
   "source": [
    "total_log_prob = sum(token_log_probs)\n",
    "print(f\"Total Log Probability: {total_log_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751c9502-42a1-4891-b37a-1c735efee800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
